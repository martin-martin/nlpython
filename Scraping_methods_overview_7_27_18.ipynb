{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "import lxml.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1.Requests and lxml.html method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1)First, use the requests library to open the webpage in Python. We could have used\n",
    "# an lxml method, but it has been noted in stackoverflow, that it doesn't work\n",
    "# with all websites, and requests seems to have bettter success. So we are using it.\n",
    "html = requests.get('https://president.yale.edu/speeches-writings/statements')\n",
    "\n",
    "# (2)So... we just openned a webpage using requests and pass the response to \"lxml.html.fromstring\" method\n",
    "doc = lxml.html.fromstring(html.content)\n",
    "#This provides us with an object of html element type.\n",
    "# This object has the xpath method which we can use to query\n",
    "#  the html document, giving us a structured way of extracting\n",
    "#  data from an html document.\n",
    "\n",
    "# (3)Now we write an xpath for extracting the tags that contains the content\n",
    "#that we want to extract. Here it is multiple links. This is where\n",
    "# mehtod #2, below, really shines. With one line of code, you can pull\n",
    "# all the links. But since we are focused on method 1 here, let's have at it\n",
    "# Well how do we know what we're after? We need to open the webpage\n",
    "# and inspect the element to understand how the content is structured,\n",
    "# which tags correspond to the data we want to grab. On the Yale site, each \n",
    "# link is written with \"a href\" tags. That's where we start.But I'm not\n",
    "#entirely sure whether \"a\" is the tag, or \"a href\".**Need to Investigate**\n",
    "# There is another way.\n",
    "# Chrome allows us to pick the element we want and \"copy\" the exact xpath\n",
    "# a long string. We do this below, and it worked. I do not know\n",
    "# whether this will grab all the tags, or just this one. **Need to investigate**\n",
    "statement_links = doc.xpath(//*[@id=\"block-system-main\"]/div/div/div/div/table/tbody/tr[4]/td[2]/a)\n",
    "# Since we want to return all of the links, let's use a generalized xpath. But I'm not sure \n",
    "# how to define it using xpath syntax. Do I use a \"div\", a \"class\" or an \"id\" here?\n",
    "# The actual element is: <a href=\"/speeches-writings/statements/yale-s-response-daca-announcement\">Yaleâ€™s Response to the DACA Announcement</a>\n",
    "# The class is <td class=\"views-field views-field-title\"> ** MY thinking is to use the class\n",
    "# here because it seems to correspond with all of the writings, not just a generalized \"links\"\n",
    "# tag. But, I'm not sure on sytax if I wanted just links. For class I am trying the following,\n",
    "# without success. Not sure what I'm doing wrong.\n",
    "#statement_links_all = doc.xpath('//div[@class=\"views-field views-field-title\"]')\n",
    "# It looks like we don't need the \"div\" above. Let's try it with just @class\n",
    "# statement_links_all = doc.xpath('//[@class=\"views-field views-field-title\"]')\n",
    "# No luck. It returns the following error:\n",
    "# Traceback (most recent call last):\n",
    "#   File \"<stdin>\", line 1, in <module>\n",
    "#   File \"src/lxml/etree.pyx\", line 1577, in lxml.etree._Element.xpath\n",
    "#   File \"src/lxml/xpath.pxi\", line 307, in lxml.etree.XPathElementEvaluator.__call__\n",
    "#   File \"src/lxml/xpath.pxi\", line 227, in lxml.etree._XPathEvaluatorBase._handle_result\n",
    "# lxml.etree.XPathEvalError: Invalid expression\n",
    "# Hmmm....okay, we may need something before the [@class, whether a 'div' or a 'section'\n",
    "# In the examples we looked at yesterday, we had both.\n",
    "# When I try this:\n",
    "# statement_links_all = doc.xpath('//div[@class=\"views-field views-field-title\"]')\n",
    "# It returns an empty list\n",
    "# * Moving on to other methods for now. I'm missing something here.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get('https://president.yale.edu/speeches-writings/statements')\n",
    "# This first command looks a lot like the requests first command. Probably because\n",
    "# requests-html package was written by the same people. Difference, session.get\n",
    "# rather than requests .get. \n",
    "links = r.html.absolute_links\n",
    "links.find('p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
